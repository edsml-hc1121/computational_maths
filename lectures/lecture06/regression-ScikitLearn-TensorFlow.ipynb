{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Regression-example\" data-toc-modified-id=\"Regression-example-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Regression example</a></span></li><li><span><a href=\"#Recreating-using-Scikit-Learn\" data-toc-modified-id=\"Recreating-using-Scikit-Learn-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Recreating using Scikit-Learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-using-the-root-mean-square-error-in-the-loss-function\" data-toc-modified-id=\"First-using-the-root-mean-square-error-in-the-loss-function-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>First using the root mean square error in the loss function</a></span></li><li><span><a href=\"#Try-with-different-metric---the-one-norm-(or-rather-the-Mean-Absolute-Error)\" data-toc-modified-id=\"Try-with-different-metric---the-one-norm-(or-rather-the-Mean-Absolute-Error)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Try with different metric - the one-norm (or rather the Mean Absolute Error)</a></span></li></ul></li><li><span><a href=\"#Recreating-using-TensorFlow\" data-toc-modified-id=\"Recreating-using-TensorFlow-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Recreating using TensorFlow</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-using-the-root-mean-square-error-in-the-loss-function\" data-toc-modified-id=\"First-using-the-root-mean-square-error-in-the-loss-function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>First using the root mean square error in the loss function</a></span></li><li><span><a href=\"#Try-with-different-metric---the-one-norm-(or-rather-the-Mean-Absolute-Error)\" data-toc-modified-id=\"Try-with-different-metric---the-one-norm-(or-rather-the-Mean-Absolute-Error)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Try with different metric - the one-norm (or rather the Mean Absolute Error)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 6\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as sl\n",
    "from pprint import pprint\n",
    "# the following allows us to plot triangles indicating convergence order\n",
    "from mpltools import annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare our linear regression results obtained using an optimisation algorithm applied to different misfit functions (which we saw in lectures 4 and 5) with results obtained using SKLearn and TensorFlow.\n",
    "\n",
    "Note that the TF code below is based on <https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/programming-exercises>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's repeat the linear regression seen in lectures.\n",
    "\n",
    "Note I'm using random data, so if you compate your results against mine, or your colleagues, then you will expect to see slightly different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invent some raw data - we will use the notation (xi,yi) for the\n",
    "# given data, where xi and yi are of length N+1 (N=len(xi)-1)\n",
    "\n",
    "\n",
    "#xi = np.linspace(0,1,10)\n",
    "#yi = xi +  0.2 * np.random.random((10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = np.linspace(0,1,10)\n",
    "yi = [0.032965, 0.248166, 0.421453, 0.38419, 0.620941, 0.692105, 0.733999, 0.873669, 1.031932, 1.133736]\n",
    "\n",
    "print(xi)\n",
    "print(yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will want to overlay a plot of the raw data a few times below so \n",
    "# let's do this via a function that we can call repeatedly\n",
    "# [Note that I've been a bit lazy in later lectures and really should\n",
    "# do this sort of thing more often to make code easier to read - apologies]\n",
    "def plot_raw_data(xi, yi, ax):\n",
    "    \"\"\"Plot x vs y on axes ax, \n",
    "    add axes labels and turn on grid\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xi : array_like\n",
    "        Array containing x data\n",
    "    yi : array_like\n",
    "        Array containing y data\n",
    "    ax :  matplotlib axes, optional\n",
    "        The axes to plot on\n",
    "    \"\"\"\n",
    "    ax.plot(xi, yi, 'ko', label='raw data')\n",
    "    ax.set_xlabel('$x$', fontsize=16)\n",
    "    ax.set_ylabel('$y$', fontsize=16)\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# For clarity we are going to add a small margin to all the plots.\n",
    "ax1.margins(0.1)\n",
    "\n",
    "# plot the raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "\n",
    "# add a figure title\n",
    "ax1.set_title('Our simple raw data', fontsize=16)\n",
    "\n",
    "# Add a legend\n",
    "ax1.legend(loc='best', fontsize=14);\n",
    "# loc='best' means we let matplotlib decide the best place for the\n",
    "# legend to go.  For other options see \n",
    "#  https://matplotlib.org/api/_as_gen/matplotlib.pyplot.legend.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a polynomial of degree 1, i.e. a straight line, to our (xi, yi) data from above\n",
    "# we'll explain what's going on here later in this lecture\n",
    "degree = 1\n",
    "poly_coeffs = np.polyfit(xi, yi, degree)\n",
    "print('poly_coeffs: ',poly_coeffs)\n",
    "\n",
    "# use poly1d to turn the coeffs into a function, p1, we can evaluate\n",
    "p1 = np.poly1d(poly_coeffs)\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "# Plot the linear fit - define 100 evenly spaced points (x) covering our\n",
    "# x extent and plot our linear polynomial evaluated at these points (p1(x))\n",
    "# of course 100 is overkill for this linear example\n",
    "x = np.linspace(0., 1, 100)\n",
    "\n",
    "ax1.plot(x, p1(x), 'b', label=r'$y = {0:.4f}x+{1:.4f}$'.format(poly_coeffs[0], poly_coeffs[1]))\n",
    "\n",
    "# Overlay raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "\n",
    "# Add a legend\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "\n",
    "# add a figure title\n",
    "ax1.set_title('Raw data and the corresponding linear best fit line', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used NumPy's polynomial fitting function which \"minimises the squared error\" <https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html>\n",
    "\n",
    "i.e. it seeks the polynomial (here we chose just a straight line) which minimises the two-norm of the errors at the locations where we have data.\n",
    "\n",
    "[Don't worry about the details too much here, we'll go over these methods in detail in later lectures].\n",
    "\n",
    "\n",
    "We can code this up ourselves using SciPy, and in doing so check that our code recreates above when we choose the two-norm, but also see what happens if we select other norms with which to define the best fitting line - we need to code it up ourselves to allow us to change the norm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def line_fit(x, line_coeffs):\n",
    "    \"\"\"\n",
    "    Calculate y values for a given line and given x values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Array containing x data\n",
    "    line_coeffs : array_like\n",
    "        Array containing gradient and constant\n",
    "\n",
    "    \"\"\"\n",
    "    return line_coeffs[0]*x + line_coeffs[1]\n",
    "\n",
    "def cost_fun(line_coeffs, x, y, norm):\n",
    "    \"\"\"Cost function using two, one or max-norm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    line_coeffs : array_like\n",
    "        Array containing gradient and constant\n",
    "    x : array_like\n",
    "        Array containing x data\n",
    "    y : array_like\n",
    "        Array containing y data\n",
    "    norm : string\n",
    "        'two' for two-norm, 'one' for one-norm, 'max' for max-norm\n",
    "    \"\"\"\n",
    "    if norm=='two':\n",
    "        return sl.norm(y - line_fit(x, line_coeffs), 2)\n",
    "    elif norm=='one':\n",
    "        return sl.norm(y - line_fit(x, line_coeffs), 1)\n",
    "    elif norm=='max':\n",
    "        return sl.norm(y - line_fit(x, line_coeffs), np.inf)\n",
    "    else:\n",
    "        raise ValueError('check your norm string')\n",
    "\n",
    "degree = 1\n",
    "poly_coeffs = np.polyfit(xi, yi, degree)\n",
    "p1 = np.poly1d(poly_coeffs)\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# polyfit line\n",
    "ax = fig.add_subplot(221)\n",
    "ax.margins(0.1)\n",
    "x = np.linspace(0., 1, 100)\n",
    "ax.plot(x, p1(x), 'b', label=r'$y = {0:.4f}x+{1:.4f}$'.format(poly_coeffs[0], poly_coeffs[1]))\n",
    "plot_raw_data(xi, yi, ax)\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set_title('polyfit  linear best fit line', fontsize=12)\n",
    "        \n",
    "# minimise two-norm cost function\n",
    "x0 = np.zeros_like(poly_coeffs)\n",
    "output = minimize(cost_fun, x0, args=(xi,yi,'two'))\n",
    "# use poly1d to turn the coeffs into a function, p1, we can evaluate\n",
    "p1 = np.poly1d(output.x)\n",
    "ax = fig.add_subplot(222)\n",
    "ax.margins(0.1)\n",
    "x = np.linspace(0., 1, 100)\n",
    "ax.plot(x, p1(x), 'b', label=r'$y = {0:.4f}x+{1:.4f}$'.format(output.x[0], output.x[1]))\n",
    "plot_raw_data(xi, yi, ax)\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set_title('Best fit (two-norm)', fontsize=12)\n",
    "\n",
    "# minimise one-norm cost function\n",
    "x0 = np.zeros_like(poly_coeffs)\n",
    "output = minimize(cost_fun, x0, args=(xi,yi,'one'))\n",
    "# use poly1d to turn the coeffs into a function, p1, we can evaluate\n",
    "p1 = np.poly1d(output.x)\n",
    "ax = fig.add_subplot(223)\n",
    "ax.margins(0.1)\n",
    "x = np.linspace(0., 1, 100)\n",
    "ax.plot(x, p1(x), 'b', label=r'$y = {0:.4f}x+{1:.4f}$'.format(output.x[0], output.x[1]))\n",
    "plot_raw_data(xi, yi, ax)\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set_title('Best fit (one-norm)', fontsize=12)\n",
    "\n",
    "# minimise max-norm cost function\n",
    "x0 = np.zeros_like(poly_coeffs)\n",
    "output = minimize(cost_fun, x0, args=(xi,yi,'max'))\n",
    "# use poly1d to turn the coeffs into a function, p1, we can evaluate\n",
    "p1 = np.poly1d(output.x)\n",
    "ax = fig.add_subplot(224)\n",
    "ax.margins(0.1)\n",
    "x = np.linspace(0., 1, 100)\n",
    "ax.plot(x, p1(x), 'b', label=r'$y = {0:.4f}x+{1:.4f}$'.format(output.x[0], output.x[1]))\n",
    "plot_raw_data(xi, yi, ax)\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set_title('Best fit (max-norm)', fontsize=12)\n",
    "\n",
    "\n",
    "plt.tight_layout(pad = 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our code recreates the `numpy.polyfit` result when we choose the two-norm. Note also that we get slightly different results when we use the one-norm or the max-norm.\n",
    "\n",
    "These results are all equally valid. The fact that `numpy.polyfit` implements the two-norm without giving us the ability to change the norm highlights that so-called \"least squares\" fitting is by far the most common approach, but there may be situations where the other norms are beneficial.\n",
    "\n",
    "Let's see what happens when we perturb a single entry - this is motivated by a situation where maybe one of our sensors failed and gave a spurious result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturb one of the entries - imagine it's corrupted or subject to major measurement error\n",
    "\n",
    "yi[8] = 2.\n",
    "\n",
    "degree = 1\n",
    "poly_coeffs = np.polyfit(xi, yi, degree)\n",
    "p1 = np.poly1d(poly_coeffs)\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# polyfit line\n",
    "ax = fig.add_subplot(221)\n",
    "ax.margins(0.1)\n",
    "x = np.linspace(0., 1, 100)\n",
    "ax.plot(x, p1(x), 'b', label=r'$y = {0:.4f}x+{1:.4f}$'.format(poly_coeffs[0], poly_coeffs[1]))\n",
    "plot_raw_data(xi, yi, ax)\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set_title('polyfit  linear best fit line', fontsize=12)\n",
    "# print out the error\n",
    "print('error in RMS case: ', sl.norm(yi - p1(xi)) / np.sqrt(len(yi)))\n",
    "\n",
    "# minimise two-norm cost function\n",
    "x0 = np.zeros_like(poly_coeffs)\n",
    "output = minimize(cost_fun, x0, args=(xi,yi,'two'))\n",
    "# use poly1d to turn the coeffs into a function, p1, we can evaluate\n",
    "p1 = np.poly1d(output.x)\n",
    "ax = fig.add_subplot(222)\n",
    "ax.margins(0.1)\n",
    "x = np.linspace(0., 1, 100)\n",
    "ax.plot(x, p1(x), 'b', label=r'$y = {0:.4f}x+{1:.4f}$'.format(output.x[0], output.x[1]))\n",
    "plot_raw_data(xi, yi, ax)\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set_title('Best fit (two-norm)', fontsize=12)\n",
    "# print out the error\n",
    "print('error in two-norm case: ', sl.norm(yi - p1(xi), 2))\n",
    "print('and if we divide this by sqrt(N) to turn this into an RMS error: ', sl.norm(yi - p1(xi), 2)/np.sqrt(len(xi)) )\n",
    "\n",
    "# minimise one-norm cost function\n",
    "x0 = np.zeros_like(poly_coeffs)\n",
    "output = minimize(cost_fun, x0, args=(xi,yi,'one'))\n",
    "# use poly1d to turn the coeffs into a function, p1, we can evaluate\n",
    "p1 = np.poly1d(output.x)\n",
    "ax = fig.add_subplot(223)\n",
    "ax.margins(0.1)\n",
    "x = np.linspace(0., 1, 100)\n",
    "ax.plot(x, p1(x), 'b', label=r'$y = {0:.4f}x+{1:.4f}$'.format(output.x[0], output.x[1]))\n",
    "plot_raw_data(xi, yi, ax)\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set_title('Best fit (one-norm)', fontsize=12)\n",
    "# print out the error\n",
    "print('error in one-norm case: ', sl.norm(yi - p1(xi), 1))\n",
    "print('and if we divide this by N to turn this into a MAE: ', sl.norm(yi - p1(xi), 1)/(len(xi)) )\n",
    "\n",
    "# minimise max-norm cost function\n",
    "x0 = np.zeros_like(poly_coeffs)\n",
    "output = minimize(cost_fun, x0, args=(xi,yi,'max'))\n",
    "# use poly1d to turn the coeffs into a function, p1, we can evaluate\n",
    "p1 = np.poly1d(output.x)\n",
    "ax = fig.add_subplot(224)\n",
    "ax.margins(0.1)\n",
    "x = np.linspace(0., 1, 100)\n",
    "ax.plot(x, p1(x), 'b', label=r'$y = {0:.4f}x+{1:.4f}$'.format(output.x[0], output.x[1]))\n",
    "plot_raw_data(xi, yi, ax)\n",
    "ax.legend(loc='best', fontsize=14)\n",
    "ax.set_title('Best fit (max-norm)', fontsize=12)\n",
    "# print out the error\n",
    "print('error in inf-norm case: ', sl.norm(yi - p1(xi), np.inf))\n",
    "\n",
    "plt.tight_layout(pad = 2.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the resulting slopes of the best fit lines, between this case with the outlier with the previous slopes without the outlier, you should see that the one-norm is by far the least impacted while the max-norm is the most impacted.\n",
    "\n",
    "These sorts of issues will be important in the modules on **Inversion & Optimisation** and **Machine Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating using Scikit-Learn\n",
    "\n",
    "Let's see if we can recreate the above slopes and intercept values using the Scikit-Learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First using the root mean square error in the loss function\n",
    "\n",
    "Following <https://scikit-learn.org/stable/modules/linear_model.html>\n",
    "\n",
    "See also <https://stackabuse.com/linear-regression-in-python-with-scikit-learn/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "\n",
    "reg.fit(xi[:, np.newaxis], yi)\n",
    "\n",
    "print('slope: ', reg.coef_[0])\n",
    "print('intercept: ', reg.intercept_)\n",
    "print('MSE: ', metrics.mean_squared_error(yi, reg.predict(xi[:, np.newaxis])))\n",
    "print('RMSE: ', np.sqrt(metrics.mean_squared_error(yi, reg.predict(xi[:, np.newaxis]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with different metric - the one-norm (or rather the Mean Absolute Error)\n",
    "\n",
    "As described here <https://scikit-learn.org/stable/auto_examples/linear_model/plot_quantile_regression.html>\n",
    "\n",
    "\"...QuantileRegressor with quantile=0.5 minimizes the mean absolute error (MAE)...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "reg = linear_model.QuantileRegressor(fit_intercept=True, quantile=0.5, alpha=0.0)\n",
    "\n",
    "reg.fit(xi[:, np.newaxis], yi)\n",
    "\n",
    "print('slope: ', reg.coef_[0])\n",
    "print('intercept: ', reg.intercept_)\n",
    "print('MAE: ', metrics.mean_absolute_error(yi, reg.predict(xi[:, np.newaxis])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating using TensorFlow\n",
    "\n",
    "Now let's see if we can get TensorFlow to recreate the \"slope\", \"intercept\" and \"misfit error\" in the case of the RMS or least squares error.\n",
    "\n",
    "In the language of ML we talk about \"weight\", \"bias\" and \"loss function\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### First using the root mean square error in the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the functions that build and train a model\n",
    "def build_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential. \n",
    "  # A sequential model contains one or more layers.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Describe the topography of the model.\n",
    "  # The topography of a simple linear regression model\n",
    "  # is a single node in a single layer. \n",
    "  model.add(tf.keras.layers.Dense(units=1, \n",
    "                                  input_shape=(1,)))\n",
    "\n",
    "  # Compile the model topography into code that \n",
    "  # TensorFlow can efficiently execute. Configure \n",
    "  # training to minimize the model's mean squared error. \n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model           \n",
    "\n",
    "\n",
    "def train_model(model, feature, label, epochs, batch_size):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Feed the feature values and the label values to the \n",
    "  # model. The model will train for the specified number \n",
    "  # of epochs, gradually learning how the feature values\n",
    "  # relate to the label values. \n",
    "  history = model.fit(x=feature,\n",
    "                      y=label,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs)\n",
    "\n",
    "  # Gather the trained model's weight and bias.\n",
    "  trained_weight = model.get_weights()[0]\n",
    "  trained_bias = model.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the \n",
    "  # rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # Gather the history (a snapshot) of each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  # Specifically gather the model's root mean \n",
    "  #squared error at each epoch. \n",
    "  rmse = hist[\"root_mean_squared_error\"]\n",
    "\n",
    "  return trained_weight, trained_bias, epochs, rmse\n",
    "\n",
    "print(\"Defined create_model and train_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "  \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "\n",
    "  # Label the axes.\n",
    "  plt.xlabel(\"feature\")\n",
    "  plt.ylabel(\"label\")\n",
    "\n",
    "  # Plot the feature values vs. label values.\n",
    "  plt.scatter(feature, label)\n",
    "\n",
    "  # Create a red line representing the model. The red line starts\n",
    "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "  x0 = 0\n",
    "  y0 = trained_bias\n",
    "  x1 = feature[-1]\n",
    "  y1 = trained_bias + (trained_weight * x1)\n",
    "  plt.plot([x0, x1], [y0, y1], c='r')\n",
    "\n",
    "  # Render the scatter plot and the red line.\n",
    "  plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, rmse):\n",
    "  \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "  plt.plot(epochs, rmse, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([rmse.min()*0.97, rmse.max()])\n",
    "  plt.show()\n",
    "\n",
    "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_feature = ([1.0, 2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0, 10.0, 11.0, 12.0])\n",
    "#my_label   = ([5.0, 8.8,  9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])\n",
    "\n",
    "my_feature = xi\n",
    "my_label   = yi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "epochs=10\n",
    "my_batch_size=10\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                         my_label, epochs,\n",
    "                                                         my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(trained_weight)\n",
    "print(trained_bias)\n",
    "print(rmse.values[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we match the results above better if we run for more epochs and try to reduce the loss function further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.005\n",
    "epochs=1000\n",
    "my_batch_size=10\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature, \n",
    "                                                         my_label, epochs,\n",
    "                                                         my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(trained_weight)\n",
    "print(trained_bias)\n",
    "print(rmse.values[-1])\n",
    "# The loss curve suggests that the model does converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with different metric - the one-norm (or rather the Mean Absolute Error)\n",
    "\n",
    "Let's check our understanding by changing the \"loss function\", specifically the metric used to calculate the misfit to data, and confirm that we can get TensorFlow to recreate our one-norm result from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the functions that build and train a model\n",
    "def build_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential. \n",
    "  # A sequential model contains one or more layers.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Describe the topography of the model.\n",
    "  # The topography of a simple linear regression model\n",
    "  # is a single node in a single layer. \n",
    "  model.add(tf.keras.layers.Dense(units=1, \n",
    "                                  input_shape=(1,)))\n",
    "\n",
    "  # Compile the model topography into code that \n",
    "  # TensorFlow can efficiently execute. Configure \n",
    "  # training to minimize the model's mean squared error. \n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
    "                loss=\"mean_absolute_error\",\n",
    "                metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "  return model           \n",
    "\n",
    "\n",
    "def train_model(model, feature, label, epochs, batch_size):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  # Feed the feature values and the label values to the \n",
    "  # model. The model will train for the specified number \n",
    "  # of epochs, gradually learning how the feature values\n",
    "  # relate to the label values. \n",
    "  history = model.fit(x=feature,\n",
    "                      y=label,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs)\n",
    "\n",
    "  # Gather the trained model's weight and bias.\n",
    "  trained_weight = model.get_weights()[0]\n",
    "  trained_bias = model.get_weights()[1]\n",
    "\n",
    "  # The list of epochs is stored separately from the \n",
    "  # rest of history.\n",
    "  epochs = history.epoch\n",
    "  \n",
    "  # Gather the history (a snapshot) of each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  # Specifically gather the model's MAE at each epoch. \n",
    "  mae = hist[\"mean_absolute_error\"]\n",
    "\n",
    "  return trained_weight, trained_bias, epochs, mae\n",
    "\n",
    "print(\"Defined create_model and train_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting functions\n",
    "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
    "  \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
    "\n",
    "  # Label the axes.\n",
    "  plt.xlabel(\"feature\")\n",
    "  plt.ylabel(\"label\")\n",
    "\n",
    "  # Plot the feature values vs. label values.\n",
    "  plt.scatter(feature, label)\n",
    "\n",
    "  # Create a red line representing the model. The red line starts\n",
    "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
    "  x0 = 0\n",
    "  y0 = trained_bias\n",
    "  x1 = feature[-1]\n",
    "  y1 = trained_bias + (trained_weight * x1)\n",
    "  plt.plot([x0, x1], [y0, y1], c='r')\n",
    "\n",
    "  # Render the scatter plot and the red line.\n",
    "  plt.show()\n",
    "\n",
    "def plot_the_loss_curve(epochs, mae):\n",
    "  \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Mean Absolute Error\")\n",
    "\n",
    "  plt.plot(epochs, mae, label=\"Loss\")\n",
    "  plt.legend()\n",
    "  plt.ylim([mae.min()*0.97, mae.max()])\n",
    "  plt.show()\n",
    "\n",
    "print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.005\n",
    "epochs=1000\n",
    "my_batch_size=10\n",
    "\n",
    "my_model = build_model(learning_rate)\n",
    "trained_weight, trained_bias, epochs, mae = train_model(my_model, my_feature, \n",
    "                                                         my_label, epochs,\n",
    "                                                         my_batch_size)\n",
    "plot_the_model(trained_weight, trained_bias, my_feature, my_label)\n",
    "plot_the_loss_curve(epochs, mae)\n",
    "\n",
    "print(trained_weight)\n",
    "print(trained_bias)\n",
    "print(mae.values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
